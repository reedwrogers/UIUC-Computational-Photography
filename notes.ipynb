{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> Computational Photography <strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### <strong>Some of the Professor's Research:</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The profesor created a algorithm to turn an image into a simple 3-D scene. \n",
    "- Other research focused on turning images into 3-D scenes/ models. \n",
    "- Inserting objects into images, and making those pasted objects look like they were in the original photograph. \n",
    "- Generating videos based off of text input (comic strips).\n",
    "- Creating 3D, AR models of construction sites (Reconstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <Strong> Some Context on Computational Photography </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depictions of people and scenes have changed a lot over the last few thousands of years (from abstract, iconography - to realism)\n",
    "    - shift to perspective, real-life scenes\n",
    "<img src=\"./Arnolfini_Portrait.png\" alt=\"Arnolfini's Portrait and Mirror\">\n",
    "- Then there was the camera: first used to help artists draw realistic scenes (the Lens Based Camera Obscura)\n",
    "    - hard to learn to draw things as they are, vs how we perceive them\n",
    "<img src=\"./Images_for_Notes/Daguerre.png\" alt=\"Daguerre\">\n",
    "- But are photos really realistic?\n",
    "    - photos can be staged (like the Iraqi photo, and touched up photos in magazines.)\n",
    "    - this is where computer graphics come in. \n",
    "        - Model the 3D model of the scene\n",
    "        - use a physics engine to render from any viewpoint\n",
    "        - hard to do well\n",
    "        - sometimes looks too shiny, too real\n",
    "        - people are hard to do, pores, wrinkles, glow\n",
    "- The realism spectrum\n",
    "    - Computer Graphics:\n",
    "        - easy to create new worlds\n",
    "        - easy to manipulate objects/ viewpoints\n",
    "        - very hard to make look realistic\n",
    "    - Photography\n",
    "        - instantly realistic\n",
    "        - easy to aquire\n",
    "        - very hard to manipulate objects/ viewpoints\n",
    "- Computational photography = the best of both worlds\n",
    "    - How can I use computational techniques to capture light in new ways?\n",
    "    - How can I use computational techniques to breathe new light into the photograph?\n",
    "    - How can I use computational techniques to synthesize and organize photo collections?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### <strong>Course Objectives</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. You will have new abilities for visual creation\n",
    "2. You will get a foundation for computer vision\n",
    "3. You will better appreciate your own visual ability \n",
    "\n",
    "<img src=\"./Images_for_Notes/Thinking.png\" alt=\"Daguerre\">\n",
    "\n",
    "4. You will have fun doing cool stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Course Projects</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hybrid images\n",
    "    - Creating an image that has signal from 2 different images (different interpretation depending on the size of the image)\n",
    "2. Image quilting for texture synthesis and transfer\n",
    "    - being able to create texture in images (face on toast)\n",
    "3. Poisson editing\n",
    "    - Picture of swimming pool + picture of a bear = blended in bear into a swimming pool\n",
    "4. Image-based lighting\n",
    "    - capturing light with mirrored ball\n",
    "5. Video alignment, sitching, editing\n",
    "    - panoramic video insertion and deletion\n",
    "6. Do something cool\n",
    "    - should be about the same scale as the previous projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Pixel and Image Filters</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image formation\n",
    "    - digital camera records light into CCD (converts photons of light into electrons)\n",
    "    - measuring the total number of photos that reach each cell\n",
    "    - the original signal could be nice and continuous (curved), but is converted to discrete and blocky for the camera\n",
    "    - this is why elements of images can be pixelated and noisy\n",
    "    - Raster image\n",
    "        - matrix representation of an image\n",
    "        - one value per pixel of the image\n",
    "    - Perception of intensity\n",
    "        - humans can be tricked by our own visual system (checkerboard shadow example)\n",
    "    - Digital Color Images\n",
    "        - using filters, CCD's can record color based on intensity \n",
    "        - all images are just three colors with different intensities (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python..\n",
    "\n",
    "import cv2\n",
    "\n",
    "im = cv2.imread(filename)\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) # orders channels as RGB\n",
    "im = im / 255 # values range from 0 to 1\n",
    "\n",
    "# RGB image im is a H x W x 3 matrix (numpy.ndarray)\n",
    "\n",
    "im[0,0,0] # top left pixel value in R-channel\n",
    "im[y, x, c] # y + 1 pixels down, x + 1 pixels to the right in the cth channel\n",
    "im[H-1, W-1, 2] # bottom right pixel in the B channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Filtering\n",
    "\n",
    "- is the compute function of local neighborhood at each position\n",
    "- Really important\n",
    "    - enhance images\n",
    "        - denoise, resize, increase contrast, etc\n",
    "    - extract information from images\n",
    "        - texture, edges, distinctive points, etc\n",
    "    - detect patterns \n",
    "        - template matching\n",
    "- box filter\n",
    "    - looks like a box in 2D plot\n",
    "    - a 3x3 matrix filter applied to an image means you take each 3x3 part of the image, and get the dot product of of each segment\n",
    "- what does a filter do?\n",
    "    - it sort of blurs out the image\n",
    "    - smoothes\n",
    "    - reduces contrast\n",
    "    - convolution (?)\n",
    "    - answer: takes the average of each window\n",
    "-  You have to get the dot product of the filter and the segment of the image you are looking at. The segment of the image matches the filter image in terms of size.\n",
    "- The resulting image is the same size as the original image, as the affected pixel during each round of the filtering operation is just the one in the middle. We handle the edges of the photo differently. \n",
    "- This process is called the filtering operation\n",
    "- What different filters do\n",
    "    - zero matrix with a 1 in the center?\n",
    "        - does nothing, as every pixel gets replaced by itself. \n",
    "        - called the identity filter\n",
    "    - A zero matrix (3x3) with a single one at position 23?\n",
    "        - shifted to the left\n",
    "    - Doubling the image (zero matrxi with 2 in the center), and then subtracting a box filter?\n",
    "        - this is a sharpening filter\n",
    "        - if you subtract a blurred image from a sharp image, you just get the sharper image\n",
    "        - making the differences in pixel intensities STRONGER\n",
    "    - Edge filter\n",
    "        - [1, 0, -1\n",
    "           2, 0, -2\n",
    "           1, 0, -1]\n",
    "        - gets the absolute value of an image\n",
    "        - sum of pixels from the left and subtracting the pixels from the right\n",
    "        - turning this filter horizontal makes a Sobel filter\n",
    "- How can we synthesize motion blur?\n",
    "    - shift the image by multiple positions and then average it out\n",
    "    - How is this done with a filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "im_fn = './Thinking.png'\n",
    "\n",
    "im = cv2.imread(im_fn)\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)/255 # convert to grayscale for now\n",
    "\n",
    "theta = 0\n",
    "len = 15\n",
    "mid = (len-1)/2\n",
    "\n",
    "fil = np.zeros((len,len))\n",
    "print(fil)\n",
    "\n",
    "fil[:, int(mid)] = 1/len\n",
    "R = cv2.getRotationMatrix2D((mid, mid),theta,1)\n",
    "fil = cv2.warpAffine(fil, R, (len,len))\n",
    "\n",
    "im_fil = cv2.filter2D(im, -1, fil)\n",
    "\n",
    "%matplotlib inline\n",
    "fig, axes = plt.subplots(3,1,figsize=(50,50))\n",
    "axes[0] = imshow(im,cmap='gray')\n",
    "axes[1] = imshow(im,cmap='gray')\n",
    "axes[2] = imshow(im,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation vs Convolution\n",
    "\n",
    "- different terms for filtering. \n",
    "- sometimes used interchangably\n",
    "- strong relationship between them though\n",
    "- correlation\n",
    "    - when you take a window over the image, you multiply corresponding elements of the window with the kernel (filter matrix)\n",
    "- convolution\n",
    "    - same as correlation, but you rotate the kernel first by 180 degrees\n",
    "    - calculated using fast fourier transforms\n",
    "- if you can do correlation, you can also do convolution\n",
    "- if you have a symetric kernel, the output will be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key properties of linear filters\n",
    "\n",
    "- linearity\n",
    "    - if you filter the sum of two signals, that is the same as filtering each separately and adding those responses together\n",
    "- shift invariance\n",
    "    - same behavior regardless of pixel location\n",
    "    - filter(shift(f)) = shift(filter(f))\n",
    "    - any linear shift invariant operator can also be represented as a convolution\n",
    "- cummutative\n",
    "    - a * b = b * a\n",
    "    - conceptually not difference between filter and signal (image)\n",
    "    - I could also filter my blur kernel with my image, and I get the same result. This is unlike matrices of linear algebra\n",
    "- associative \n",
    "    - a * (b * c) = (a * b) * c\n",
    "    - often apply several filters one after another\n",
    "    - this is equivalent to applying one filter\n",
    "- distributes over addition \n",
    "    - a * (b + c) = (a * b) + (a * c)\n",
    "- scalars factor out\n",
    "- identity filter is a filter with a 1 in the center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important filter: Gaussian\n",
    "\n",
    "- 4 representations as depicted in the lecture\n",
    "- effective smoother without edgey artifacts (compared to box filter)\n",
    "- remove \"high frequency\" components from the image (low pass filter)\n",
    "    - images become more smooth\n",
    "- if you convolve a gaussian with another gaussian you get another gaussian. (Ring a bell from stats? Normal distribution + normal distribution = normal distribution)\n",
    "    - convolving twice with a gaussian kernal of width sigma is the same as convolving once with a kernel of width sigma * radical 2. \n",
    "- separable\n",
    "    - you can divide it into a product of two 1-D Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separability \n",
    "\n",
    "To summarize what the professor said here, separability just describes the fact that we can split our filter into smaller filters. The example given was splitting a 3x3 filter into the two 1x3 and 3x1 filters that could be multiplied to produce it. Using these two to convolve on the image instead of the larger 3x3 matrix is much faster for larger matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some practical matters\n",
    "\n",
    "- How big should a filter be?\n",
    "    - values at edges should be near zero\n",
    "    - rule of thumb for Gaussian: set kernel half-width to >= 3*sigma (since Gaussian is not discrete and can't be zero)\n",
    "    - this just says that if the standard deviation of the pixel values is 1, then we want the size of the filter to be 7 by 7 (3 sigma is 3, so we want 3 on one side, 3 on the other, 1 value in the middle)\n",
    "    - too small of a size on the Gaussian results in what is essentially the box filter\n",
    "- What about near the edge?\n",
    "    - the filter window falls off the edge of the image\n",
    "    - need to extrapolate - aka making the image larger such that our filter can fit.\n",
    "    - methods (all can be done in Python)\n",
    "        - clipping (black filter around the whole image)\n",
    "        - wrap around \n",
    "        - copy edge\n",
    "        - reflect across edge (DEFAULT)\n",
    "    - What is the size of the output?\n",
    "        - full (response is the size of original image plus what we extrapolated)\n",
    "        - same (response is the size of original image) (DEFAULT)\n",
    "        - valid (original image does not get padded at all, just record response where filter fits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application Representing Texture\n",
    "\n",
    "- regular or stochastic patterns caused by bumps, grooves, and/ or markings\n",
    "- How can we represent texture?\n",
    "    - computre respones of blobs and edges at various orientations and scales\n",
    "    - filter bank = set of filters\n",
    "    - we can apply multiple filters to an image and measure the responses of each one to see how much of an impact that filter made on the image\n",
    "    - the result is a vector to describe the image\n",
    "        - this tells us something about the texture of the image\n",
    "        - for example, what would it mean if we saw there was a high response to verticle filters in an image, low responses to horizontal filters, and low responses to blob filters (blob meaning what it sounds like, organic circular looking patterns)?\n",
    "            - probably means we have an image with a vertical looking texture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Images (Project 1)\n",
    "\n",
    "- a way of combining two images so that you get a different perception of the image depending on your distance to the image\n",
    "- Gaussian filtered image (smooth image) + laplacian filtered image (detail image) = hybrid image\n",
    "- far away and small = blurred image\n",
    "- close and large = detail image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "- images are a matrix of numbers\n",
    "- linear fitering is the dot product at each window position of the image with the filter (kernel)\n",
    "- be aware of details (size of filter, extrapolation, cropping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fourier transform and frequency domain\n",
    "    - another way to look at images\n",
    "    - frequency view of filtering\n",
    "    - another look at hybrid images\n",
    "    - sampling\n",
    "- Why does the Gaussian give a nice smooth image, but the box filter gives edgey artifacts?\n",
    "    - hard to understand from the spacial domain, easier with frequency domain\n",
    "- Why does we get different distance-dependent interpretations of hybrid images?\n",
    "    - also answered in the frequency domain\n",
    "- Why does a lower resolution image still make sense to us? What do we lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Jean Baptiste Fourier </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crazy idea\n",
    "    - any univariate function can be rewritten as a weighted sum of sines and cosines of different frequencies\n",
    "    - Laplace, Lagrange, Poisson were the judges\n",
    "        - not impressed\n",
    "    - his work was not even translated to english until 70 years later\n",
    "- Idea: you can compose a signal out of sines and cosines\n",
    "    - Amplitude * sin((frequency * x) + phase)\n",
    "    - convergence on the square wave with enough smaller frequencies added\n",
    "- We often think of frequencies in terms of music\n",
    "    - pitches\n",
    "- images are usually looked at in the spacial domain, but we can also look at them with frequencies\n",
    "- in two dimensions:\n",
    "    - fourier images are always symetric about the origin \n",
    "    - dots close to center = low frequency, slow change\n",
    "    - dots farther away = higher frequency, faster change\n",
    "    - signals can be composed and added together just like in the spacial domain\n",
    "- Fourier transform\n",
    "    - stores the  magnitude and phase at each frequency\n",
    "        - magnitude encodes how much signal there is at a particular frequency\n",
    "        - phase encodes spatial information (indirectly) - how sine and cosines are shifted\n",
    "- can compute a few ways, including Euler's formula\n",
    "- can compute the transform as an integral (continuous) or as a sum (discrete)\n",
    "- Fast Fourier transformation is what we use\n",
    "- The Convolutional Theorem\n",
    "    - why it works\n",
    "    - the Fourier transform of the convolution of two functions is the product of their Fourier transforms\n",
    "        - F[g * h] = F[g]F[h]\n",
    "    - the inverse Fourier transform of the product of two Fourier transforms is the convolution of the two inverse Fourier transforms\n",
    "        - F inverse [gh] = F inverse [g] * F inverse [h]\n",
    "    - what does this mean?\n",
    "        - <strong>Convolution in the spacial domain is equivalent to multiplication in the frequency domain </strong>\n",
    "- Properties of Fourier Tranforms\n",
    "    - linearity\n",
    "    - the Fourier transform of a real signal is symmetric about the origin\n",
    "    - The energy of the signal is the same as the energy of its Fourier transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Filtering with FFT </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images_for_Notes/Filtering_FFT.png\" alt=\"FFT Filtering\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering with FFT in Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def filter_image(im, fil):\n",
    "    # im: H x W floating point numpy ndarray representing image in grayscale\n",
    "    # fil: M x M floating point numpy ndarray representing 2D filter\n",
    "\n",
    "    H,W = im.shape\n",
    "    hs = fil.shape[0] // 2                          # half of filter size\n",
    "    fftsize = 1024                                  # should be order of 2 (for speed) and include padding\n",
    "    im_fft = np.fft.fft2(im, (fftsize,fftsize))     # 1) fft im with padding\n",
    "    fil_fft = np.fft.fft2(fil, (fftsize, fftsize))  # 2) fft fil, pad to same size as image\n",
    "    im_fil_fft = im_fft * fil_fft                   # 3) multiply fft images\n",
    "    im_fil = np.fft.ifft2(im_fil_fft)               # 4) inverse fft2\n",
    "    im_fil = im_fil[hs:hs + H, hs:hs + W]           # 5) remove padding\n",
    "    im_fil = np.reak(im_fil)                        # 6) extract out real part\n",
    "    return im_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying with fft\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def display_frequency_image(frequency_image):\n",
    "    '''\n",
    "    frequency_image: H x W floating point numpy ndarray representing image after FFT\n",
    "\n",
    "    in grayscale\n",
    "\n",
    "    '''\n",
    "    shifted_image = np.fft.fftshift(frequency_image)\n",
    "    amplitude_image = np.abs(shifted_image)\n",
    "    log_amplitude_image = np.log(amplitude_image)\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(log_amplitude_image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Which has more information, the phase or the magnitude? </strong>\n",
    "\n",
    "- magnitude = amount of power in frequencies\n",
    "- phase = how they are shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FFT and decompose to magnitude and phase\n",
    "im1_fft = fft2(im1);\n",
    "im1_fft_mag = abs(im1_fft);\n",
    "im1_fft_phase = angle(im1_fft);\n",
    "im2_fft = fft2(im2);\n",
    "im2_fft_mag = abs(im2_fft);\n",
    "im2_fft_phase = angle(im2_fft);\n",
    "# Combine mag and phase from different images and compute inverse FFT\n",
    "mag1_phase2 = ifft2(im1_fft_mag.*cos(im2_fft_phase)+1i*im1_fft_mag.*sin(im2_fft_phase));\n",
    "phase1_mag2 =ifft2(im2_fft_mag.*cos(im1_fft_phase)+1i*im2_fft_mag.*sin(im1_fft_phase));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- phase contains more information it would appear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Answering some questions </strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so why does the Gaussian give smooth images and the box gives edgey artifacts?\n",
    "    - The gaussian preserves information only in the low frequencies\n",
    "    - The square does the same, but also in some isolated spots of higher frequency. These spots cause the artifacts\n",
    "- Why does lower resolution still make sense to us, and what do we lose?\n",
    "    - in all the natural image frequency plots we saw, power is always really concetrated in the lower frequencies (a red dot in the center, meaning there is a lot of power in the low freqs, while blue out towards the rest of the plot indicating not much power)\n",
    "        - AKA: this means there is not a lot of change as you go from a pixel to its neighbors \n",
    "            - images are mostly smooth\n",
    "        - what you lose is the high frequency information, but there is not a lot to begin with, so therefore we keep a lot of the useful information \n",
    "- How do you shrink an image?\n",
    "    - Naively, you might think you could just throw away every other row and column (to reduce by a factor of two)\n",
    "        - This causes an aliasing problem\n",
    "            - can be dangerous and cause artifacts\n",
    "                - wagon wheels rolling in the wrong way in movies\n",
    "                - checkerboards disintegrate in ray tracing\n",
    "                - striped shirts look funny on color tv\n",
    "    - Nyquist-Shannon Sampling Theorem\n",
    "        - when sampling a singal at discrete intervals, the sampling frequency must be >= 2 x f_max\n",
    "        - f_max = max frequency of the input signal\n",
    "        - This will allow you to reconstruct the original perfectly from the sampled version\n",
    "    - anti-aliasing\n",
    "        - sample more often (doesn't achieve downsampling goal really)  \n",
    "        - OR, get rid of all frequencies that are greater than half the new sampling frequency\n",
    "            - will lose information\n",
    "            - but it is better than aliasing\n",
    "            - apply a smoothing filter\n",
    "        - Algorithm\n",
    "            - start with image\n",
    "            - apply low pass filter (Gaussian)\n",
    "            - sample every other pixel\n",
    "- Why does a lower resolution image still make sense to us? What do we lose?\n",
    "    - because it preserves low frequency (make sure you apply a low pass filter)\n",
    "- Why do we get different, distance-dependent interpretations of hybrid images?\n",
    "    - Early processing in humans filters for various orientations and scales of frequency\n",
    "    - perceptual cues in the mid frequencies dominate perception\n",
    "    - when we see an image from far away, we sare basically subsampling it\n",
    "        - thus, we don't have access to high frequencies (or even mid frequencies)\n",
    "    - Hybrid image = low passed image + high passed image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Summary </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sometimes it makes sense to think of images and filtering in the frequency domain\n",
    "    - Fourier analysis\n",
    "- can be faster to filter using FFT for large images (N LogN vs N^2 for auto-correlation)\n",
    "- Images are mostly smooth\n",
    "    - basis for compression\n",
    "- remember to low-pass before you down sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Templates and Image Pyramids</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- applications of filtering\n",
    "- Template matching\n",
    "    - useful for object recognition\n",
    "        - how would we find an eye in the image?\n",
    "            - evaluate whether or not our eye filter (the representation of the eye) matches the segement of the inage it is on\n",
    "    - correlation\n",
    "        - just take the eye patch as a filter, and compute the filter response\n",
    "            - resulted in a fuzzy image instead of clear eyes and blury elsewhere. WHy?\n",
    "                - can partly fix this by subtracting the filter mean intensity. Results in a mostly black image with a couple of detections where eyes could be. False detections included\n",
    "        - could use the sum of squared difference\n",
    "            - take the template, and scan it over the image\n",
    "            - at each position, take the difference between the corresponding pixels and the template\n",
    "            - square those values and sum them \n",
    "\n",
    "<img src=\"./Images_for_Notes/Matching_w_filters.png\" alt=\"Filtering\">\n",
    "\n",
    "- SSD requires that you have an exact match in contrast and intensity\n",
    "- normalized cross correlation\n",
    "\n",
    "<img src=\"./ncc.png\" alt=\"NCC\">\n",
    "\n",
    "- makes it so that any value added to the image (constant in frequency, intensity), then it gets canceled out\n",
    "- best method to use?\n",
    "    - it depends:\n",
    "    - for zero mean filter\n",
    "        - fastest, but not good matcher\n",
    "    - SSD\n",
    "        - next fastest\n",
    "        - sensitive to overall sensitivity\n",
    "    - Normalized cross corelation\n",
    "        - slowest\n",
    "        - cannot be implemented with linear filters, because you are subtracting a different mean value for each image patch\n",
    "        - but it is invariant to local average intensity and contrast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Template matching at scale </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what if we want to detect larger or smaller eyes?\n",
    "    - image pyramid\n",
    "- Review of sampling\n",
    "    - means I am extracting out every other pixel or a subset of the pixels to create a lower resolution image\n",
    "    - first do a smoothing filter to kill off high frequencies that won't be preserved anyway\n",
    "    - then sample from that filtered image\n",
    "- Doing this iteratively creates a Gaussian pyramid\n",
    "    - then, run this set of images (all different scales) against the image to find different sized objects\n",
    "- there is also the Laplacian pyramid\n",
    "<img src=\"./Images_for_Notes/laplacian.png\" alt=\"Laplacian Pyramid\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Coarse-to-fine Image Registration </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Idea\n",
    "\n",
    "1. Compute Gaussian pyramid\n",
    "2. align with coarse pyramid\n",
    "    - find minimum SSD position\n",
    "3. Successively align with finer pyramids\n",
    "    - search small range centered around position determined at coarser scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images_for_Notes/ctf.png\" alt=\"ctf\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this saying? Basically, we can use the smaller image (with less area to search) to locate things, and then project that up on each level of our pyramid, only having to search a small area around where our last estimate was. \n",
    "\n",
    "Why is this faster?\n",
    "    - you only need to search a small range of values in finer resolution images. Just match the coarse image and then project that using your last estimate\n",
    "    - you are not guaranteed to get the same result if the pattern in high resolution is noticable, but less so in the coarse image\n",
    "    - it can actually help you locate the area better because high frequencies are more prone to noise that could be wrong but provide a correct signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Denoising, Compression </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you align the images using the FFT?\n",
    "    - yes, in the same way that you can do linear filtering with FFT\n",
    "- How is it that a 4MP image can be compressed to a few hunred KB without a noticeable change?\n",
    "    - as we know, most of the intensities are in the lower frequencies. Compressing removes the high frequencies\n",
    "    - JPEG compression\n",
    "        - Lossy image compression\n",
    "        - Idea: extract 8x8 patches in the image and compress them independently using discrete cosine transform\n",
    "            - basis - set of basis functions for the 8x8 patch\n",
    "            - the first coefficient B(0,0) is the DC component, the average intensity\n",
    "            - the top left coefficients represent low frequencies, the bottom right, high frequencies\n",
    "            - Compression\n",
    "                - quantize\n",
    "                    - more coarsely for high frequencies (which also tend to have lower values)\n",
    "                    - many quantized high frequency values will be zero\n",
    "                - encode\n",
    "                    - can decode with inverse dct\n",
    "        - Summary\n",
    "            - convert image to YCrCb\n",
    "            - subsample color by factor of 2\n",
    "                - people have bad resolution for color\n",
    "            - split into blocks (8x8 typically), subtract 128 (so average intensity is closer to 0)\n",
    "            - For each block\n",
    "                - compute DCT coefficients\n",
    "                - coarsely quantize\n",
    "                    - many high frequency components will become 0\n",
    "                - encode (with Huffman coding, for example)\n",
    "    - PNG compression\n",
    "        - lossless, can exactly get back original signal\n",
    "        - main idea\n",
    "            - predict a pixel's value based on uper left neighborhood\n",
    "            - store difference of predicted and actual value\n",
    "            - pkzip it (DEFLATE algorithm)\n",
    "        - works well with a small number of images\n",
    "- Denoising\n",
    "    - additive Gaussian noise\n",
    "        - denoised with a Gaussian filter\n",
    "    - reducing salt-and-pepper noise by Gaussian smoothing does not really work\n",
    "        - reduced/ removed with median filtering (outliers are removed)\n",
    "    - median filtering\n",
    "        - returns the median value of the local neighborhood\n",
    "        - operates over a window by selecting the median intensity in the window\n",
    "        - nonlinear: sorting is not linear\n",
    "        - Median filtering is much more robost to Outliers compared to Gaussian\n",
    "        - not ideal for Gaussian noise, but good for removing noise but preserving edges\n",
    "    - weighted median\n",
    "        - pixels further from center count less\n",
    "    - clipped mean (average, ignoring few brighteset and darkest pixels)\n",
    "    - Bilateral filtering \n",
    "        - third most important after Gaussian filter and Median filter\n",
    "        - weight by spatial distance (distance from center, like the Gaussian) AND intensity difference\n",
    "        - will simply check what other pixels are similar to the center pixel, those get more weight, as well as the pixels that are close to the center pixel (USE THIS ONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Light and Color and the Eye </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the color we perceive is a function of multiple factors\n",
    "    - material\n",
    "    - geometry\n",
    "    - light source\n",
    "- the eye\n",
    "    - cameras are built to get light in a similar way to the eye\n",
    "    - the human eye is a camera\n",
    "        - iris\n",
    "            - colored annulus with radial muscles\n",
    "        - pupil\n",
    "            - the hole (aperture) whose size is controlled by the iris\n",
    "        - retina\n",
    "            - the \"film\"\n",
    "            - area on the back filled with rods and cones\n",
    "        - cones\n",
    "            - cone-shaped\n",
    "            - less sensitive\n",
    "            - operate in high light \n",
    "            - color vision\n",
    "        - rods\n",
    "            - rod shaped\n",
    "            - highly sensitive\n",
    "            - operate at night\n",
    "            - gray-scale vision\n",
    "            - slower to respond\n",
    "    - eyes can deal with several orders of magnitude of intensity\n",
    "    - cones = daylight\n",
    "    - rods = moonlight\n",
    "    - distribution of rods and cones\n",
    "        - cones spike at about 0 degrees\n",
    "        - only a small range of angles where we have a lot of cones\n",
    "            - meaning we have high resolution of image in only a location of where we look\n",
    "    - rod distribution\n",
    "        - broader distribution\n",
    "        - rods are more sensitive off center of our vision\n",
    "- the electromagnetic spectrum\n",
    "    - we see between 400 and 700 nanometers (400 is red, 700 is blue)\n",
    "- the physics of light\n",
    "    - represented with a spectrogram\n",
    "        - intensity of light that is received at each wavelength of light\n",
    "    - laser\n",
    "        - monochromatic\n",
    "    - normal daylight\n",
    "        - large range of the visible spectrum\n",
    "        - white = roughly even across\n",
    "        - yellowish = more skewed towards higher spectrum colors\n",
    "    - metamers\n",
    "        - different white flowers can have different spectrums of light\n",
    "            - we only perceive a single color, not the whole spectrum\n",
    "    - physiology of color vision\n",
    "        - 3 kinds of cones\n",
    "            - short (blue)\n",
    "            - medium\n",
    "            - long (red)\n",
    "        - more cones are red and green than blue\n",
    "        - medium and long cones have peaks at similar wavelengths\n",
    "            - because human eyes evolved to see more colors of green (plants) and red (fruit)\n",
    "        - why do we have 3 cones?\n",
    "            - 3 is better than 2\n",
    "            - more cones = better able to differentiate patters of light\n",
    "            - more cones = worse in darkness\n",
    "        - M and L on the X chromosone\n",
    "            - men are more likely to be colorblind \n",
    "        - Women are more likely to be telechromatic (see color spectrums better)\n",
    "        - why dont' we perceive a spectrum (or even RGB)?\n",
    "            - we perceive\n",
    "                - hue: mean wavelength, color\n",
    "                - saturation: variance, vividness\n",
    "                - intensity: total amount of light\n",
    "            - same perceived color can be recreated with combinations of three primary colors (trichomacy)\n",
    "        - trichromacy\n",
    "            - theres a small amount of wavelengths not represented by RGB\n",
    "            - CIE-XYZ\n",
    "                - matrix multiplication on RBG intensities (XYZ)\n",
    "        - consequence:\n",
    "            - we can display every wavelength perfectly with RGB monitors\n",
    "            - you can reproduce the same pattern of light in our perception using just 3 receptors\n",
    "        - Bayer grid\n",
    "            - put a set of filters in front of the photoreceptor cells\n",
    "                - each pixel measures amount of red or blue or green light\n",
    "                - each cell gets a range, peak will be in red  blue or green\n",
    "            - since eyes have more green and red, so does the Bayer grid\n",
    "        - alternative to Bayer\n",
    "            - RGB + W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> How is light reflected from a surface? </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depends on\n",
    "    - illumination properties: wavelength, orientation, intensity\n",
    "    - surface properties: material, surface orientation, roughness, etc.\n",
    "- Lambertian surface\n",
    "    - some light is absorbed (function of albedo)\n",
    "    - remaining light is reflected in all directions (diffuse reflection)\n",
    "    - examples: soft cloth, concrete, matte paints\n",
    "- Diffuse reflection\n",
    "    - intensity DOES depend on illumination angle because less light comes in at oblique angles\n",
    "    - this explains why two surfaces of a building are different colors\n",
    "        - the side more directly facing the light source is brighter\n",
    "        - in this case, differences in intensity tell us about the rectangular shape of the building\n",
    "    - the perceived intensity does NOT depend on the viewer angle\n",
    "        - amount of reflected light propertional to cos(theta)\n",
    "        - visible solid angle also proportional to cos(theta)\n",
    "- specular reflection\n",
    "    - direct reflection \n",
    "    - mirrors are fully specular\n",
    "        - light does not scatter when hitting a mirror. It just reflects at the given angle\n",
    "    - specularities = bright spots on a shiny object\n",
    "        - easter eggs or hardwood floor reflection\n",
    "- BRDF: bidirectional reflectance distribution function \n",
    "    - allows us to model completely how a surface reflects light\n",
    "    - given the direction that the light came into a surface, and the direction light is emmitted from the surface\n",
    "        - given orientation of the surface\n",
    "            - and wavelength of light\n",
    "                - what fraction of light will be reflected? This answers the question\n",
    "- more complex effects\n",
    "    - transparency\n",
    "    - refraction\n",
    "    - flourescence\n",
    "    - phosphorescence\n",
    "    - subsurface scattering\n",
    "    - interreflection\n",
    "- The color of objects\n",
    "    - the color of the light source\n",
    "    - the color of the surface\n",
    "- Color constancy\n",
    "    - we want to know that the reflection function of the object is, or its light with a white light\n",
    "    - we don't want to know the RGB value\n",
    "- Shadows\n",
    "    - a point that can't see the source at all is \"in shadow\"\n",
    "    - for point sources, the geometry is simple\n",
    "    - shading and shadows give major cues to shape and position\n",
    "- Summary\n",
    "    - light has a spectrum of wavelengths\n",
    "    - observed light depends on \n",
    "        - illumination intensities\n",
    "        - surface orientation\n",
    "        - material (albed, specular component, diffuse component)\n",
    "    - every object is an indirect light source for every other\n",
    "    - shading and shadows are informative about shape and position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Color Spaces </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RGB is the default space\n",
    "    - some drawbacks\n",
    "        - strongly correlated channels\n",
    "        - non-perceptual\n",
    "            - we don't look at a color image and think about %'s of red, green, or blue\n",
    "- More perceptual color spaces\n",
    "    - XYZ covers all natural light, RGB covers most\n",
    "    - XYZ is the basis for other spaces\n",
    "    - perceptual uniformity\n",
    "        - the idea is to try to represent color in a way so that the distance between colors (in vector representation of that color) corresponds roughly to the perceptual distance between the colors\n",
    "        - measured by asking people to tune two colors to make them the same\n",
    "        - used to make the CIE L*a*b* space\n",
    "            - perceptually uniform color space\n",
    "            - more meaningful perceptually as it is \n",
    "            - L channel: luminance\n",
    "            - A channel: difference between red and gree\n",
    "            - B channel: difference between yellow and blue\n",
    "        - lumincance = brightness\n",
    "        - chrominance = color (2 of these in LAB)\n",
    "        - more information in intensity than in color\n",
    "    - HSV\n",
    "        - hue saturation and value\n",
    "            - hue: color of the rainbow\n",
    "                - non linear function of RGB space\n",
    "            - saturation: brightness of the color\n",
    "                - non linear function of RGB space\n",
    "            - value: high value is bright, low value is dark\n",
    "                - intensity image\n",
    "        - not the easeiest to work in\n",
    "            - cyclical \n",
    "    - YCbCr\n",
    "        - used by tv communication\n",
    "        - good for compression\n",
    "        - similar to LAB (1 lum, 2 chrom channels)\n",
    "        - can be computed with a linear operation\n",
    "            - just a matrix operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Color balancing </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- important ideas\n",
    "    - typical images are gray on avaerge; this can be used to detect distortions\n",
    "    - larger differences are more visible, so using the full intensity range improves visibility\n",
    "    - it's often easier to work in non-RGB color space (convert to YCrCb first and imcrease the Y channel, maybe)\n",
    "- linear adjustment\n",
    "    - easy way to balance color\n",
    "    - multiply R, G, and B channels by some constant\n",
    "        - multiply all elements in that channel (matrix multiplication)\n",
    "    - how do we chose constants?\n",
    "        - pick ones that do not change overall intensity\n",
    "            - some get smaller, some get larger\n",
    "            - use the gray world assumption: the image average should be gray\n",
    "        - white balancing\n",
    "            - choose a reference as the white or gray color (see demo)\n",
    "    - better to balance in camera's RGB (linear) than display RGB (non-linear)\n",
    "- tone mapping\n",
    "    - typical problem: compress values from a high range to a smaller range\n",
    "        - most cameras do this\n",
    "        - camera captures 12-bit linear intensity and needs to compress to 8 bits\n",
    "    - challenge: there are some really bright parts and some really dark parts\n",
    "        - we want to see the contrast in both parts\n",
    "        - solution: do a non-linear mapping\n",
    "            - Reinhart operator\n",
    "            - take the luminance in the world divided by 1 + luminance in the world\n",
    "                - small values are preserved, and the large values are compressed a larger and larger rate \n",
    "                - really compressed bright values, less so on darker ones\n",
    "- point processing\n",
    "    - apply a function on each pixel intensity to map it to a new value\n",
    "    - gamma adjustment\n",
    "        - for every pixel value, the gamma adjustment is just that pixel raised to a value gamma\n",
    "            - values need to be between 0 and 1 first\n",
    "            - low gamma = brighter\n",
    "            - higher gamma = darker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Histogram Equalization </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what if we have an image where our image has parts that too dark and too bright, or gamma values squished in the middle\n",
    "- histogram equalization \n",
    "    - a method to remap intensity values so that values use the full range of the image as much as possible\n",
    "- histogram\n",
    "    - count of how many values are observed in a vector\n",
    "- cummulative histogram\n",
    "    - cummulative sum of a histogram\n",
    "    - sum of all values less than or equal to a given value\n",
    "- algorithm\n",
    "    - given an image with pixel values from 0 to 255, specify a function that remaps pixel values, so that new values are more broadly distributed\n",
    "    - first, compute the cumulative histogram\n",
    "    - f(i) will then be getting the percentile of the intensity value * 255. multiply by alpha to do some balancing between the original image and the new image. \n",
    "        - alpha = 1 is just the histogram equalization\n",
    "    - alpha is just how strong of an effect you want from the histogram equalization\n",
    "- locally adaptive histogram equalization\n",
    "    - just do normal HE but in different grid cells of the image\n",
    "    - assign a given pixel a weighted average value with more weight on local grids\n",
    "- what about HE in color?\n",
    "    - you don't want to apply to RGB channels\n",
    "        - it will apply them separately and give a weird color shift\n",
    "    - first, break down the image into a luminance channel, and chrominance channels\n",
    "    - perform locally adaptive HE on the luminance channel\n",
    "    - then, just hsv2rgb to convert the chrominance channels back to RGB\n",
    "    - kind of looks like it might be overdoing it, adds new shadows unless you average with original image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Texture Synthesis </strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- New section of topics called \"the digital canvas\"\n",
    "    - cutting pasting warping filling blending\n",
    "- Texture synthesis\n",
    "    - taking a pattern and generating more of it\n",
    "- hole filling\n",
    "    - process of filling in the background behind holes in images \n",
    "- Texture\n",
    "    - depicting spacially repeating pattersn\n",
    "    - appear naturally and frequently\n",
    "- goal of texture synthesis\n",
    "    - create new samples of a given texture\n",
    "    - many applications\n",
    "        - virtual environments\n",
    "        - hole filling\n",
    "        - texturing surfaces\n",
    "- The challenge\n",
    "    - some patterns are highly structured, and some are more stochastic\n",
    "        - a plaid shirt vs tile floor vs clouds (white noise)\n",
    "- One idea: build probability distributions\n",
    "    - thought: if you can model how the textures vary locally, then you can regenerate that image according to that model\n",
    "    - Basic idea:\n",
    "        - compute statistics of input texture\n",
    "        - generate a new texture that keeps those same statistics\n",
    "    - problem: it just doesn't work very well\n",
    "        - hard to model using probability \n",
    "        - works well for smaller repeating textures\n",
    "        - harder for longer repeating ones (like marble)\n",
    "- other idea: instead of trying to explicitly model the probabilty distribution of intensities, just sample from the image\n",
    "    - represent the texture empiraclly by just sampling different patches from that texture\n",
    "    - first, make a Markov assumption: every pixel is independent of all the other pixels given its neighborhood\n",
    "    - so, if we want to find what intensity should be at position p, then we should search the input image for all similar neighborhoods\n",
    "    <img src=\"./brick.png\" alt=\"brick\">\n",
    "- Idea is from Shannon (information theory)\n",
    "    - generate English-sounding sentences by modeling the probability of each word given the previous word\n",
    "    - large n will result in more structured senetences, but these sentences are more predictable as there may be less instances to pull from in training, resulting in the same word appearing every time\n",
    "- This in in natural language, but it is a similar idea to the image application\n",
    "    - instead of a set of words, we model a set of pixels\n",
    "    - How do we match the patches? In other words, how do we find the patch in the image that we will use to pull a sample from (similar patch)?\n",
    "        - Gaussian SSD\n",
    "            - basically says we care a lot about the pixels near the pixel we are trying to match, and we care a little about the ones farther away\n",
    "    - what order to fill in the new pixels?\n",
    "        - onion skin order\n",
    "            - pixels with the most neighbors are synthesized first\n",
    "        - to synthesize from scratch, start with a randomly seleted small patch from the source texture\n",
    "    - how big should patches be?\n",
    "        - depends\n",
    "        - <img src=\"./Images_for_Notes/circles.png\" alt=\"circle\">\n",
    "        - basically, the smaller the patch, the more stochastic (random) your texture\n",
    "        - larger would be more ridgid and repeated\n",
    "- Texture synthesis algorithm\n",
    "    - While image not filled in\n",
    "        - get unfilled pixels with filled neighbors, sorted by the number of filled neighbors\n",
    "        - basically, take all the pixels that you want to fill, put them in a queue, and their priority in that queue is based on how many neighbors are already filled\n",
    "            - then pop off top pixel of the queue\n",
    "                - extract the patch around this pixel\n",
    "                - locate the top N patch matches from the input image\n",
    "                    - then, randomly choose one of these N matches\n",
    "                    - copy the intensity of the center of that patch to fill in the pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Hole-Filling </strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- goes from outside in in stages\n",
    "- Extrapolation\n",
    "    - increase the size of an image\n",
    "    - very simple algorithm\n",
    "        - but able to reproduce patterns well\n",
    "- In-painting natural scenes\n",
    "    - there may be structures in the image that you want to preserve\n",
    "    - the onion filling order can sometimes fail here..\n",
    "    - Order of filling matters\n",
    "    - gradient-sensitive works better (think of the triangle example)\n",
    "        - give more weight that are part of continuous lines\n",
    "    - Filling order\n",
    "        - fill a pixel that..\n",
    "            1. Is surrounded by other known pixels\n",
    "            2. is a continuation of a strong gradient or edge \n",
    "    - Summary\n",
    "        - the Efros and Leung texture synthesis algorithm\n",
    "            - very simple\n",
    "            - surprisingly good results\n",
    "            - synthesis is easier than analysis\n",
    "            - ..but very slow\n",
    "- Image Quilting (Efros and Freeman)\n",
    "    - next iteration on texture synthesis\n",
    "    - observation\n",
    "        - neighbor pixels are highly correlated\n",
    "    - Idea: unit of synthesis = block\n",
    "        - exactly the same but now we want P(B | N(B))\n",
    "        - much faster: synthesize all pixels in a block at once\n",
    "    - but of complication in blending patches together\n",
    "        - random placement of blocks leaves lots of seams\n",
    "        - neighboring blocks constrained by overlap still creates some seams\n",
    "        - minimal error boundary cut\n",
    "            - best way to prevent seams\n",
    "            - basic idea: you want to cut through the pixels that are similar in the two blocks to make a seamless transition between the two\n",
    "            - solving for minimum cut path\n",
    "                - represent it as a graph and find the shortest path through the graph\n",
    "                - Use Djikstra's, O(m), where m is the number of columns in the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Texture Transfer </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like trying to explain one object with bits and pieces of another object\n",
    "- combine a potato with an orange\n",
    "- compose a face out of a bread texture\n",
    "- Idea is extremely similar to image synthesis in general, but with an extra guidance function\n",
    "    - we want to match with a target image\n",
    "    - take target image, convert to grayscale\n",
    "    - take texture, grayscale and blur\n",
    "    - generate a new texture such that the overlapping patches of the texture match well, but also the blurred luminance matches the blurred luminance of the input image\n",
    "        - darker around darker areas of face, lighter near lighter areas\n",
    "- source texture should have a range of intensities \n",
    "- PatchMatch\n",
    "    - more efficient search\n",
    "        - randomly initialize matches\n",
    "        - see if neighbor's offsets are better\n",
    "        - randomly search a local window for better matches\n",
    "        - repeat 3, 4 across image several times\n",
    "- Related Idea: image analogies\n",
    "    - turn an image into a more painted version of itself\n",
    "    - idea: define similarity between A and B\n",
    "        - a patch from A is similar to a patch from B if they have similar luminance values\n",
    "        - for each patch in B, find a patch that matches A who's corresponding match in A' also fits well with existing patches in B'\n",
    "        - algorithm is done iteratively, coarse to fine\n",
    "- Pix2Pix\n",
    "    - image to image translation\n",
    "    - very general method\n",
    "    - finds patches similar from its training data to fill in result\n",
    "    - 2 guiding principles\n",
    "        - supervised objective\n",
    "        - try to trick the discriminator\n",
    "- Cycle GAN\n",
    "    - very general concept\n",
    "    - zebra to horse, horse to zebra\n",
    "- <strong>Things to remember</strong>\n",
    "    - texture synthesis and hole-filling can be thought of as a form of probabilistic hallucination\n",
    "        - instead of expliciting modeling probability distributions, we say \"I have seen something like this surrounding area before, take it from my memory and fill in the gap\"\n",
    "    - simple, similarity based matching is a powerful tool\n",
    "        - synthesis \n",
    "        - hole filling\n",
    "        - transfer \n",
    "        - artistic filtering\n",
    "        - super resolution\n",
    "        - recognition\n",
    "    - key is how to define similarity and efficiently find neighbors\n",
    "    - new methods are able to create much more complicated synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Cutting Images </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- finding seams and boundaries\n",
    "    - could be segmentation, where the user gives us an idea of which picture they want removed or cut out\n",
    "    - could be retargeting, changing the width or height of an image without just squishing it in a direction\n",
    "    - could be stitching, putting together two images\n",
    "- fundamental concept\n",
    "    - think of the image as a graph \n",
    "    - intelligent scissors: good boundary = short path\n",
    "    - graph cuts: good region has low cutting cost\n",
    "- Semi-automated segmentation\n",
    "    - user provides imprecise and incomplete specification of region, algorithm has to read his/ her mind\n",
    "    - key problems\n",
    "        - what groups of pixels form cohesive regions?\n",
    "        - what pixels are likely to be on the boundary of regions?\n",
    "        - what region is the user trying to to select?\n",
    "    - what makes a good region?\n",
    "        - similar colors\n",
    "        - similar texture\n",
    "        - in the foreground\n",
    "        - smooth\n",
    "        - sharpness\n",
    "        - distinction from the the background\n",
    "    - what makes a good boundary?\n",
    "        - edges of the image\n",
    "        - high gradient along the boundary\n",
    "        - gradient in right direction\n",
    "        - smooth\n",
    "- The image as a graph\n",
    "    - node: pixel\n",
    "    - edge: cost of path or cut between two pixels\n",
    "- Intelligent scissors\n",
    "    - 24 years old, but still used\n",
    "    - main idea: the user can click a point on the boundary of the object and move their cursor around the object\n",
    "        - the algorithm will continually define the boundary between the seed point and the cursor\n",
    "    - a good image boundary: one that has a short path through the graph\n",
    "        - start point is seed point\n",
    "        - end point is the cursor\n",
    "        - so it is finding a path between them continiously\n",
    "    - Formulation\n",
    "        - find a good boundary between seed points\n",
    "    - challenges\n",
    "        - minimize interaction time\n",
    "        - define what makes a good boundary\n",
    "        - efficiently find it \n",
    "    - method (4 steps)\n",
    "        1. Define boundary cost between neighboring pixels\n",
    "        2. user specifies a starting point (seed)\n",
    "        3. Compute lowest cost from seed to each other pixel\n",
    "        4. get path from seed to cursor, choose new seed, repeat\n",
    "    - Define boundary cost between neighboring pixels\n",
    "        - 3 terms\n",
    "            a. lower if edge is present (canny edge detector)\n",
    "            b. lower if gradient is strong\n",
    "            c. lower if gradient is in direction of boundary\n",
    "    - User specifies a starting point (seed)\n",
    "        - snapping\n",
    "    - Compute lowest cost from seed to each other pixel\n",
    "        - Djikstra's shortest path algorithm\n",
    "    - then get new seed, get path between seeds, repeat\n",
    "    - improvements?\n",
    "        - snapping when placing first seed\n",
    "        - automatically adjust to boundary as user drags\n",
    "        - freeze stable boundary points to make new seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Graph Cut-Based Segmentation </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- magic wand\n",
    "    - selects pixels based on color similarity\n",
    "- grab cut\n",
    "    - combines both magic wand and intelligent scissors\n",
    "- graph cut\n",
    "    1. define graph\n",
    "        - usually 4-connected or 8-connected\n",
    "    2. set weights to the foreground/ background\n",
    "        - color histogram or mixture of Gaussians for background and foreground\n",
    "        - can tell us the probability of being in the foreground or background given a pixel color\n",
    "            - if pixel color is more likely for one or other, it has a bias to be assined there (this is the unary potential)\n",
    "    3. set weights for edges between pixels\n",
    "    4. apply the min-cut/ max-flow algorithm\n",
    "    5. return to 2, using current labels to compute foreground, background models (conversion after 2/3 implementations)\n",
    "- lazy snapping\n",
    "    - one line color for foreground, one for background\n",
    "- the best algorithm combines a box with this lazy snapping\n",
    "- limitations of graph cuts\n",
    "    - requires associative graphs\n",
    "        - connected nodes should prefer to have the same label\n",
    "    - is optimal only for binary problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Seam Carving </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same for boundary finding of intelligent scissors and texture synthesis\n",
    "- keeps interesting content and removes uninteresting content instead of just uniformally shrinking the image\n",
    "- find the shortest path that is smooth, aka locating areas of the image with not a lot of variation. \n",
    "    - remove these\n",
    "- Stiching\n",
    "    - find shortest path from top to bottom (or left to right), where cost = gradient magnitude\n",
    "    - ideal boundary\n",
    "        - similar color in both images\n",
    "        - high gradient in both images\n",
    "        - key idea: cut through areas that have similar color and both have high gradients (lots of changes)\n",
    "            - makes edge artifacts less noticable\n",
    "- Summary of big ideas:\n",
    "    - treat image as a graph\n",
    "        - pixels are nodes\n",
    "        - between pixel edge weights based on gradients\n",
    "        - sometimes per-pixel weights for affinity to foreground/ background\n",
    "    - good boundaries are a short path through the graph\n",
    "    - good regions are produced by a low-cost cut "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
