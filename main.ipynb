{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong> Computational Photography <strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### <strong>Some of the Professor's Research:</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The profesor created a algorithm to turn an image into a simple 3-D scene. \n",
    "- Other research focused on turning images into 3-D scenes/ models. \n",
    "- Inserting objects into images, and making those pasted objects look like they were in the original photograph. \n",
    "- Generating videos based off of text input (comic strips).\n",
    "- Creating 3D, AR models of construction sites (Reconstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <Strong> Some Context on Computational Photography </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depictions of people and scenes have changed a lot over the last few thousands of years (from abstract, iconography - to realism)\n",
    "    - shift to perspective, real-life scenes\n",
    "<img src=\"./Arnolfini_Portrait.png\" alt=\"Arnolfini's Portrait and Mirror\">\n",
    "- Then there was the camera: first used to help artists draw realistic scenes (the Lens Based Camera Obscura)\n",
    "    - hard to learn to draw things as they are, vs how we perceive them\n",
    "<img src=\"./Daguerre.png\" alt=\"Daguerre\">\n",
    "- But are photos really realistic?\n",
    "    - photos can be staged (like the Iraqi photo, and touched up photos in magazines.)\n",
    "    - this is where computer graphics come in. \n",
    "        - Model the 3D model of the scene\n",
    "        - use a physics engine to render from any viewpoint\n",
    "        - hard to do well\n",
    "        - sometimes looks too shiny, too real\n",
    "        - people are hard to do, pores, wrinkles, glow\n",
    "- The realism spectrum\n",
    "    - Computer Graphics:\n",
    "        - easy to create new worlds\n",
    "        - easy to manipulate objects/ viewpoints\n",
    "        - very hard to make look realistic\n",
    "    - Photography\n",
    "        - instantly realistic\n",
    "        - easy to aquire\n",
    "        - very hard to manipulate objects/ viewpoints\n",
    "- Computational photography = the best of both worlds\n",
    "    - How can I use computational techniques to capture light in new ways?\n",
    "    - How can I use computational techniques to breathe new light into the photograph?\n",
    "    - How can I use computational techniques to synthesize and organize photo collections?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### <strong>Course Objectives</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. You will have new abilities for visual creation\n",
    "2. You will get a foundation for computer vision\n",
    "3. You will better appreciate your own visual ability \n",
    "\n",
    "<img src=\"./Thinking.png\" alt=\"Daguerre\">\n",
    "\n",
    "4. You will have fun doing cool stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Course Projects</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hybrid images\n",
    "    - Creating an image that has signal from 2 different images (different interpretation depending on the size of the image)\n",
    "2. Image quilting for texture synthesis and transfer\n",
    "    - being able to create texture in images (face on toast)\n",
    "3. Poisson editing\n",
    "    - Picture of swimming pool + picture of a bear = blended in bear into a swimming pool\n",
    "4. Image-based lighting\n",
    "    - capturing light with mirrored ball\n",
    "5. Video alignment, sitching, editing\n",
    "    - panoramic video insertion and deletion\n",
    "6. Do something cool\n",
    "    - should be about the same scale as the previous projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Pixel and Image Filters</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image formation\n",
    "    - digital camera records light into CCD (converts photons of light into electrons)\n",
    "    - measuring the total number of photos that reach each cell\n",
    "    - the original signal could be nice and continuous (curved), but is converted to discrete and blocky for the camera\n",
    "    - this is why elements of images can be pixelated and noisy\n",
    "    - Raster image\n",
    "        - matrix representation of an image\n",
    "        - one value per pixel of the image\n",
    "    - Perception of intensity\n",
    "        - humans can be tricked by our own visual system (checkerboard shadow example)\n",
    "    - Digital Color Images\n",
    "        - using filters, CCD's can record color based on intensity \n",
    "        - all images are just three colors with different intensities (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python..\n",
    "\n",
    "import cv2\n",
    "\n",
    "im = cv2.imread(filename)\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) # orders channels as RGB\n",
    "im = im / 255 # values range from 0 to 1\n",
    "\n",
    "# RGB image im is a H x W x 3 matrix (numpy.ndarray)\n",
    "\n",
    "im[0,0,0] # top left pixel value in R-channel\n",
    "im[y, x, c] # y + 1 pixels down, x + 1 pixels to the right in the cth channel\n",
    "im[H-1, W-1, 2] # bottom right pixel in the B channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Filtering\n",
    "\n",
    "- is the compute function of local neighborhood at each position\n",
    "- Really important\n",
    "    - enhance images\n",
    "        - denoise, resize, increase contrast, etc\n",
    "    - extract information from images\n",
    "        - texture, edges, distinctive points, etc\n",
    "    - detect patterns \n",
    "        - template matching\n",
    "- box filter\n",
    "    - looks like a box in 2D plot\n",
    "    - a 3x3 matrix filter applied to an image means you take each 3x3 part of the image, and get the dot product of of each segment\n",
    "- what does a filter do?\n",
    "    - it sort of blurs out the image\n",
    "    - smoothes\n",
    "    - reduces contrast\n",
    "    - convolution (?)\n",
    "    - answer: takes the average of each window\n",
    "-  You have to get the dot product of the filter and the segment of the image you are looking at. The segment of the image matches the filter image in terms of size.\n",
    "- The resulting image is the same size as the original image, as the affected pixel during each round of the filtering operation is just the one in the middle. We handle the edges of the photo differently. \n",
    "- This process is called the filtering operation\n",
    "- What different filters do\n",
    "    - zero matrix with a 1 in the center?\n",
    "        - does nothing, as every pixel gets replaced by itself. \n",
    "        - called the identity filter\n",
    "    - A zero matrix (3x3) with a single one at position 23?\n",
    "        - shifted to the left\n",
    "    - Doubling the image (zero matrxi with 2 in the center), and then subtracting a box filter?\n",
    "        - this is a sharpening filter\n",
    "        - if you subtract a blurred image from a sharp image, you just get the sharper image\n",
    "        - making the differences in pixel intensities STRONGER\n",
    "    - Edge filter\n",
    "        - [1, 0, -1\n",
    "           2, 0, -2\n",
    "           1, 0, -1]\n",
    "        - gets the absolute value of an image\n",
    "        - sum of pixels from the left and subtracting the pixels from the right\n",
    "        - turning this filter horizontal makes a Sobel filter\n",
    "- How can we synthesize motion blur?\n",
    "    - shift the image by multiple positions and then average it out\n",
    "    - How is this done with a filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "im_fn = './Thinking.png'\n",
    "\n",
    "im = cv2.imread(im_fn)\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)/255 # convert to grayscale for now\n",
    "\n",
    "theta = 0\n",
    "len = 15\n",
    "mid = (len-1)/2\n",
    "\n",
    "fil = np.zeros((len,len))\n",
    "print(fil)\n",
    "\n",
    "fil[:, int(mid)] = 1/len\n",
    "R = cv2.getRotationMatrix2D((mid, mid),theta,1)\n",
    "fil = cv2.warpAffine(fil, R, (len,len))\n",
    "\n",
    "im_fil = cv2.filter2D(im, -1, fil)\n",
    "\n",
    "%matplotlib inline\n",
    "fig, axes = plt.subplots(3,1,figsize=(50,50))\n",
    "axes[0] = imshow(im,cmap='gray')\n",
    "axes[1] = imshow(im,cmap='gray')\n",
    "axes[2] = imshow(im,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation vs Convolution\n",
    "\n",
    "- different terms for filtering. \n",
    "- sometimes used interchangably\n",
    "- strong relationship between them though\n",
    "- correlation\n",
    "    - when you take a window over the image, you multiply corresponding elements of the window with the kernel (filter matrix)\n",
    "- convolution\n",
    "    - same as correlation, but you rotate the kernel first by 180 degrees\n",
    "    - calculated using fast fourier transforms\n",
    "- if you can do correlation, you can also do convolution\n",
    "- if you have a symetric kernel, the output will be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key properties of linear filters\n",
    "\n",
    "- linearity\n",
    "    - if you filter the sum of two signals, that is the same as filtering each separately and adding those responses together\n",
    "- shift invariance\n",
    "    - same behavior regardless of pixel location\n",
    "    - filter(shift(f)) = shift(filter(f))\n",
    "    - any linear shift invariant operator can also be represented as a convolution\n",
    "- cummutative\n",
    "    - a * b = b * a\n",
    "    - conceptually not difference between filter and signal (image)\n",
    "    - I could also filter my blur kernel with my image, and I get the same result. This is unlike matrices of linear algebra\n",
    "- associative \n",
    "    - a * (b * c) = (a * b) * c\n",
    "    - often apply several filters one after another\n",
    "    - this is equivalent to applying one filter\n",
    "- distributes over addition \n",
    "    - a * (b + c) = (a * b) + (a * c)\n",
    "- scalars factor out\n",
    "- identity filter is a filter with a 1 in the center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important filter: Gaussian\n",
    "\n",
    "- 4 representations as depicted in the lecture\n",
    "- effective smoother without edgey artifacts (compared to box filter)\n",
    "- remove \"high frequency\" components from the image (low pass filter)\n",
    "    - images become more smooth\n",
    "- if you convolve a gaussian with another gaussian you get another gaussian. (Ring a bell from stats? Normal distribution + normal distribution = normal distribution)\n",
    "    - convolving twice with a gaussian kernal of width sigma is the same as convolving once with a kernel of width sigma * radical 2. \n",
    "- separable\n",
    "    - you can divide it into a product of two 1-D Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separability \n",
    "\n",
    "To summarize what the professor said here, separability just describes the fact that we can split our filter into smaller filters. The example given was splitting a 3x3 filter into the two 1x3 and 3x1 filters that could be multiplied to produce it. Using these two to convolve on the image instead of the larger 3x3 matrix is much faster for larger matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some practical matters\n",
    "\n",
    "- How big should a filter be?\n",
    "    - values at edges should be near zero\n",
    "    - rule of thumb for Gaussian: set kernel half-width to >= 3*sigma (since Gaussian is not discrete and can't be zero)\n",
    "    - this just says that if the standard deviation of the pixel values is 1, then we want the size of the filter to be 7 by 7 (3 sigma is 3, so we want 3 on one side, 3 on the other, 1 value in the middle)\n",
    "    - too small of a size on the Gaussian results in what is essentially the box filter\n",
    "- What about near the edge?\n",
    "    - the filter window falls off the edge of the image\n",
    "    - need to extrapolate - aka making the image larger such that our filter can fit.\n",
    "    - methods (all can be done in Python)\n",
    "        - clipping (black filter around the whole image)\n",
    "        - wrap around \n",
    "        - copy edge\n",
    "        - reflect across edge (DEFAULT)\n",
    "    - What is the size of the output?\n",
    "        - full (response is the size of original image plus what we extrapolated)\n",
    "        - same (response is the size of original image) (DEFAULT)\n",
    "        - valid (original image does not get padded at all, just record response where filter fits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application Representing Texture\n",
    "\n",
    "- regular or stochastic patterns caused by bumps, grooves, and/ or markings\n",
    "- How can we represent texture?\n",
    "    - computre respones of blobs and edges at various orientations and scales\n",
    "    - filter bank = set of filters\n",
    "    - we can apply multiple filters to an image and measure the responses of each one to see how much of an impact that filter made on the image\n",
    "    - the result is a vector to describe the image\n",
    "        - this tells us something about the texture of the image\n",
    "        - for example, what would it mean if we saw there was a high response to verticle filters in an image, low responses to horizontal filters, and low responses to blob filters (blob meaning what it sounds like, organic circular looking patterns)?\n",
    "            - probably means we have an image with a vertical looking texture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Images (Project 1)\n",
    "\n",
    "- a way of combining two images so that you get a different perception of the image depending on your distance to the image\n",
    "- Gaussian filtered image (smooth image) + laplacian filtered image (detail image) = hybrid image\n",
    "- far away and small = blurred image\n",
    "- close and large = detail image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "- images are a matrix of numbers\n",
    "- linear fitering is the dot product at each window position of the image with the filter (kernel)\n",
    "- be aware of details (size of filter, extrapolation, cropping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fourier transform and frequency domain\n",
    "    - another way to look at images\n",
    "    - frequency view of filtering\n",
    "    - another look at hybrid images\n",
    "    - sampling\n",
    "- Why does the Gaussian give a nice smooth image, but the box filter gives edgey artifacts?\n",
    "    - hard to understand from the spacial domain, easier with frequency domain\n",
    "- Why does we get different distance-dependent interpretations of hybrid images?\n",
    "    - also answered in the frequency domain\n",
    "- Why does a lower resolution image still make sense to us? What do we lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Jean Baptiste Fourier </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crazy idea\n",
    "    - any univariate function can be rewritten as a weighted sum of sines and cosines of different frequencies\n",
    "    - Laplace, Lagrange, Poisson were the judges\n",
    "        - not impressed\n",
    "    - his work was not even translated to english until 70 years later\n",
    "- Idea: you can compose a signal out of sines and cosines\n",
    "    - Amplitude * sin((frequency * x) + phase)\n",
    "    - convergence on the square wave with enough smaller frequencies added\n",
    "- We often think of frequencies in terms of music\n",
    "    - pitches\n",
    "- images are usually looked at in the spacial domain, but we can also look at them with frequencies\n",
    "- in two dimensions:\n",
    "    - fourier images are always symetric about the origin \n",
    "    - dots close to center = low frequency, slow change\n",
    "    - dots farther away = higher frequency, faster change\n",
    "    - signals can be composed and added together just like in the spacial domain\n",
    "- Fourier transform\n",
    "    - stores the  magnitude and phase at each frequency\n",
    "        - magnitude encodes how much signal there is at a particular frequency\n",
    "        - phase encodes spatial information (indirectly) - how sine and cosines are shifted\n",
    "- can compute a few ways, including Euler's formula\n",
    "- can compute the transform as an integral (continuous) or as a sum (discrete)\n",
    "- Fast Fourier transformation is what we use\n",
    "- The Convolutional Theorem\n",
    "    - why it works\n",
    "    - the Fourier transform of the convolution of two functions is the product of their Fourier transforms\n",
    "        - F[g * h] = F[g]F[h]\n",
    "    - the inverse Fourier transform of the product of two Fourier transforms is the convolution of the two inverse Fourier transforms\n",
    "        - F inverse [gh] = F inverse [g] * F inverse [h]\n",
    "    - what does this mean?\n",
    "        - <strong>Convolution in the spacial domain is equivalent to multiplication in the frequency domain </strong>\n",
    "- Properties of Fourier Tranforms\n",
    "    - linearity\n",
    "    - the Fourier transform of a real signal is symmetric about the origin\n",
    "    - The energy of the signal is the same as the energy of its Fourier transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Filtering with FFT </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Filtering_FFT.png\" alt=\"FFT Filtering\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering with FFT in Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def filter_image(im, fil):\n",
    "    # im: H x W floating point numpy ndarray representing image in grayscale\n",
    "    # fil: M x M floating point numpy ndarray representing 2D filter\n",
    "\n",
    "    H,W = im.shape\n",
    "    hs = fil.shape[0] // 2                          # half of filter size\n",
    "    fftsize = 1024                                  # should be order of 2 (for speed) and include padding\n",
    "    im_fft = np.fft.fft2(im, (fftsize,fftsize))     # 1) fft im with padding\n",
    "    fil_fft = np.fft.fft2(fil, (fftsize, fftsize))  # 2) fft fil, pad to same size as image\n",
    "    im_fil_fft = im_fft * fil_fft                   # 3) multiply fft images\n",
    "    im_fil = np.fft.ifft2(im_fil_fft)               # 4) inverse fft2\n",
    "    im_fil = im_fil[hs:hs + H, hs:hs + W]           # 5) remove padding\n",
    "    im_fil = np.reak(im_fil)                        # 6) extract out real part\n",
    "    return im_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying with fft\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def display_frequency_image(frequency_image):\n",
    "    '''\n",
    "    frequency_image: H x W floating point numpy ndarray representing image after FFT\n",
    "\n",
    "    in grayscale\n",
    "\n",
    "    '''\n",
    "    shifted_image = np.fft.fftshift(frequency_image)\n",
    "    amplitude_image = np.abs(shifted_image)\n",
    "    log_amplitude_image = np.log(amplitude_image)\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(log_amplitude_image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Which has more information, the phase or the magnitude? </strong>\n",
    "\n",
    "- magnitude = amount of power in frequencies\n",
    "- phase = how they are shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FFT and decompose to magnitude and phase\n",
    "im1_fft = fft2(im1);\n",
    "im1_fft_mag = abs(im1_fft);\n",
    "im1_fft_phase = angle(im1_fft);\n",
    "im2_fft = fft2(im2);\n",
    "im2_fft_mag = abs(im2_fft);\n",
    "im2_fft_phase = angle(im2_fft);\n",
    "# Combine mag and phase from different images and compute inverse FFT\n",
    "mag1_phase2 = ifft2(im1_fft_mag.*cos(im2_fft_phase)+1i*im1_fft_mag.*sin(im2_fft_phase));\n",
    "phase1_mag2 =ifft2(im2_fft_mag.*cos(im1_fft_phase)+1i*im2_fft_mag.*sin(im1_fft_phase));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- phase contains more information it would appear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Answering some questions </strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so why does the Gaussian give smooth images and the box gives edgey artifacts?\n",
    "    - The gaussian preserves information only in the low frequencies\n",
    "    - The square does the same, but also in some isolated spots of higher frequency. These spots cause the artifacts\n",
    "- Why does lower resolution still make sense to us, and what do we lose?\n",
    "    - in all the natural image frequency plots we saw, power is always really concetrated in the lower frequencies (a red dot in the center, meaning there is a lot of power in the low freqs, while blue out towards the rest of the plot indicating not much power)\n",
    "        - AKA: this means there is not a lot of change as you go from a pixel to its neighbors \n",
    "            - images are mostly smooth\n",
    "        - what you lose is the high frequency information, but there is not a lot to begin with, so therefore we keep a lot of the useful information \n",
    "- How do you shrink an image?\n",
    "    - Naively, you might think you could just throw away every other row and column (to reduce by a factor of two)\n",
    "        - This causes an aliasing problem\n",
    "            - can be dangerous and cause artifacts\n",
    "                - wagon wheels rolling in the wrong way in movies\n",
    "                - checkerboards disintegrate in ray tracing\n",
    "                - striped shirts look funny on color tv\n",
    "    - Nyquist-Shannon Sampling Theorem\n",
    "        - when sampling a singal at discrete intervals, the sampling frequency must be >= 2 x f_max\n",
    "        - f_max = max frequency of the input signal\n",
    "        - This will allow you to reconstruct the original perfectly from the sampled version\n",
    "    - anti-aliasing\n",
    "        - sample more often (doesn't achieve downsampling goal really)  \n",
    "        - OR, get rid of all frequencies that are greater than half the new sampling frequency\n",
    "            - will lose information\n",
    "            - but it is better than aliasing\n",
    "            - apply a smoothing filter\n",
    "        - Algorithm\n",
    "            - start with image\n",
    "            - apply low pass filter (Gaussian)\n",
    "            - sample every other pixel\n",
    "- Why does a lower resolution image still make sense to us? What do we lose?\n",
    "    - because it preserves low frequency (make sure you apply a low pass filter)\n",
    "- Why do we get different, distance-dependent interpretations of hybrid images?\n",
    "    - Early processing in humans filters for various orientations and scales of frequency\n",
    "    - perceptual cues in the mid frequencies dominate perception\n",
    "    - when we see an image from far away, we sare basically subsampling it\n",
    "        - thus, we don't have access to high frequencies (or even mid frequencies)\n",
    "    - Hybrid image = low passed image + high passed image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Summary </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sometimes it makes sense to think of images and filtering in the frequency domain\n",
    "    - Fourier analysis\n",
    "- can be faster to filter using FFT for large images (N LogN vs N^2 for auto-correlation)\n",
    "- Images are mostly smooth\n",
    "    - basis for compression\n",
    "- remember to low-pass before you down sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
